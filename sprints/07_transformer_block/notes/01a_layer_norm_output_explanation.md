# Explanation of `01_layer_norm_example.py` Output

This note explains the output generated by running the `sprints/07_transformer_block/results/01_layer_norm_example.py` script.

```
Input Tensor Shape: torch.Size([4, 10, 64])

Initialized LayerNorm: LayerNorm((64,), eps=1e-05, elementwise_affine=True)
Learnable gamma (weight) shape: torch.Size([64])
Learnable beta (bias) shape: torch.Size([64])

Output Tensor Shape: torch.Size([4, 10, 64])

--- Verification for sample 0, position 0 ---
Mean across features: 0.000000 (Expected: ~0)
Std Dev across features: 0.999995 (Expected: ~1)

--- Verification for sample 2, position 5 ---
Mean across features: 0.000000 (Expected: ~0)
Std Dev across features: 0.999994 (Expected: ~1)

--- No Affine Transformation ---
LayerNorm without affine parameters: LayerNorm((64,), eps=1e-05, elementwise_affine=False)
```

## Breakdown

1.  **`Input Tensor Shape: torch.Size([4, 10, 64])`**

    - This shows the shape of our randomly generated input tensor.
    - It represents a batch of `4` sequences, each of length `10`, where each item in the sequence (e.g., a token embedding) has `64` features (`d_model`).

2.  **`Initialized LayerNorm: LayerNorm((64,), eps=1e-05, elementwise_affine=True)`**

    - This prints the initialized `nn.LayerNorm` module.
    - `(64,)`: This is the `normalized_shape`. It confirms that LayerNorm is configured to calculate the mean and standard deviation across the last dimension, which has size 64 (our `d_model`).
    - `eps=1e-05`: This is the small value $\epsilon$ added to the denominator during normalization for numerical stability (to prevent division by zero if the variance is tiny). This is the default value.
    - `elementwise_affine=True`: This indicates that the LayerNorm module _will_ create and learn the scale ($\\gamma$, called `weight` in PyTorch) and shift ($\\beta$, called `bias` in PyTorch) parameters. This is the default behavior.

3.  **`Learnable gamma (weight) shape: torch.Size([64])`**

    - This shows the shape of the learnable scale parameter $\gamma$ (the `weight` attribute of the `LayerNorm` module).
    - Its size is `64`, matching the `normalized_shape`, because each feature dimension gets its own scale factor.

4.  **`Learnable beta (bias) shape: torch.Size([64])`**

    - This shows the shape of the learnable shift parameter $\beta$ (the `bias` attribute).
    - Its size is also `64`, matching the `normalized_shape`, as each feature dimension gets its own shift factor.

5.  **`Output Tensor Shape: torch.Size([4, 10, 64])`**

    - This confirms that applying Layer Normalization does _not_ change the shape of the tensor. It operates element-wise across the specified dimension(s).

6.  **`--- Verification for sample 0, position 0 ---`** (and similarly for sample 2, position 5)

    - Here, we take the output tensor and look at a single vector (the features for the token at position 0 within the first sequence in the batch).
    - **`Mean across features: 0.000000 (Expected: ~0)`**: We calculate the mean of the 64 features _for this specific token_. As expected, after Layer Normalization (and before any learned scaling/shifting, since $\gamma$ starts at 1 and $\beta$ starts at 0), the mean is extremely close to 0.
    - **`Std Dev across features: 0.999995 (Expected: ~1)`**: We calculate the standard deviation of the 64 features _for this specific token_. As expected, it's extremely close to 1.
    - This verification confirms that LayerNorm successfully normalized the features _within_ that specific `(sample, position)` vector to have zero mean and unit variance.

7.  **`--- No Affine Transformation ---`**
    - **`LayerNorm without affine parameters: LayerNorm((64,), eps=1e-05, elementwise_affine=False)`**: This shows the configuration of a `LayerNorm` module initialized with `elementwise_affine=False`. Notice it's identical to the previous one, except for this flag.
    - A module initialized like this would _not_ have the learnable `weight` ($\gamma$) or `bias` ($\beta$) parameters. Its output would simply be the normalized values $\hat{x}_i$, without the final scaling and shifting step $y_i = \gamma_i \hat{x}_i + \beta_i$.
