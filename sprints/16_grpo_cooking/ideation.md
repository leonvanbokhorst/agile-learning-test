# Step-by-Step Plan: GRPO Fine-Tuning of Llama3.2 3B for Chain-of-Thought Reasoning

## 1. Setting Up the RLHF Training Environment 
Prepare the local machine (Windows 11 + WSL2 with NVIDIA RTX 4090) for reinforcement learning with human feedback (RLHF) training:
- **WSL2 Ubuntu Installation:** Install an Ubuntu distribution in WSL2 and update it (e.g. `sudo apt update && sudo apt upgrade`). Ensure WSL2 has access to the NVIDIA GPU by installing the latest NVIDIA drivers on Windows and the CUDA toolkit on WSL2 ([Meta debuts slimmed-down Llama models for low-powered devices - SiliconANGLE](https://siliconangle.com/2024/10/24/meta-debuts-slimmed-llama-models-low-powered-devices/#:~:text=models%20more%20accessible%20with%20the,powered%20devices)). This allows PyTorch to utilize the 4090 GPU from within WSL.
- **Python & Virtual Env:** Install Python 3.x (ideally 3.9+). Create a virtual environment or Conda environment in WSL2 for the project to isolate dependencies.
- **PyTorch (GPU Enabled):** Inside WSL, install PyTorch with CUDA support. For example, use the pip wheel for CUDA 11.8 or 12 (`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`). Verify that `torch.cuda.is_available()` returns True.
- **Hugging Face Libraries:** Install Hugging Face Transformers, Datasets, and Accelerate for model and data handling (`pip install transformers datasets accelerate`). These libraries provide convenient APIs for model loading and data processing.
- **TRL (Transformer RL) Library:** Install the Hugging Face TRL library (`pip install trl`) for ready-to-use implementations of RL algorithms like PPO/GRPO ([huggingface/trl: Train transformer language models with ... - GitHub](https://github.com/huggingface/trl#:~:text=huggingface%2Ftrl%3A%20Train%20transformer%20language%20models,to%20train%20Deepseek%20AI%27s%20R1)). The TRL library includes a `GRPOTrainer` specifically implementing Group Relative Policy Optimization ([huggingface/trl: Train transformer language models with ... - GitHub](https://github.com/huggingface/trl#:~:text=GRPOTrainer%20implements%20the%20Group%20Relative,to%20train%20Deepseek%20AI%27s%20R1)), which will simplify our training loop.
- **Other Tools:** Install any other needed libraries: `numpy` for numerical operations, `bitsandbytes` for 8-bit optimizations if needed, and `evaluate` for evaluation metrics. Also install `tqdm` for progress bars and `rich` or `tensorboard` for logging training progress.
- **GPU Optimization Settings:** Plan to use mixed-precision to leverage the 4090's capabilities. Enable FP16 or BF16 training via Hugging Face Accelerate or PyTorch autocast to reduce memory usage and speed up training. Techniques like gradient accumulation (to simulate larger batches) and gradient checkpointing (to save memory by not storing all intermediate activations) can help if memory becomes a bottleneck. If needed, consider low-rank adaptation (LoRA) or 8-bit quantization to reduce memory while fine-tuning ([Meta debuts slimmed-down Llama models for low-powered devices - SiliconANGLE](https://siliconangle.com/2024/10/24/meta-debuts-slimmed-llama-models-low-powered-devices/#:~:text=Meta%E2%80%99s%20researchers%20said%20they%20used,precision%20environments)). For instance, QLoRA (quantization-aware LoRA) was used by Meta to efficiently fine-tune Llama 3.2 models in low precision ([Meta debuts slimmed-down Llama models for low-powered devices - SiliconANGLE](https://siliconangle.com/2024/10/24/meta-debuts-slimmed-llama-models-low-powered-devices/#:~:text=Meta%E2%80%99s%20researchers%20said%20they%20used,precision%20environments)).
- **Pitfalls to Avoid:** Ensure the CUDA toolkit in WSL matches the PyTorch version (mismatched CUDA versions can cause runtime errors). Double-check that WSL2 GPU support is enabled (`wsl --update` on Windows if necessary). Be mindful of WSL2 memory limits – configure `.wslconfig` if needed to allocate enough RAM for training. Always test a small model forward pass on the GPU to confirm everything is set up correctly before proceeding.

## 2. Loading and Preparing the Llama3.2 3B Model
Set up the base language model that will serve as the policy:
- **Model Selection:** Use Meta’s **Llama 3.2 3B** model as the starting point. This is a 3-billion parameter LLM from the Llama 3.2 family ([meta-llama/Llama-3.2-3B - Hugging Face](https://huggingface.co/meta-llama/Llama-3.2-3B#:~:text=The%20Llama%203,in%201B%20and%203B%20sizes)). It’s the latest small-sized Llama, designed for strong performance even on limited hardware, and available in pretrained and instruction-tuned variants. For our use case (chain-of-thought reasoning), the *instruction-tuned* version (Llama-3.2-3B-Instruct) is preferred since it has been trained to follow prompts and should generate coherent answers.
- **Accessing the Model:** Log in to Hugging Face CLI (`huggingface-cli login`) if needed, as the Llama 3.2 weights may require acceptance of a license. Then use the Transformers API to download and load the model:
  ```python
  from transformers import AutoTokenizer, AutoModelForCausalLM
  tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
  model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B-Instruct", device_map="auto", torch_dtype="auto")
  ```
  This will fetch the model and tokenizer. The `device_map="auto"` will automatically put the model on the GPU (and CPU if needed for layers, but a single 4090 can host a 3B model fully in VRAM). Using `torch_dtype="auto"` loads in float16 by default (for efficiency).
- **Model Configuration:** Verify the model’s generation settings. Set the pad token and special tokens as needed (for Llama, the tokenizer may not have a pad token by default; you can set `tokenizer.pad_token = tokenizer.eos_token` for safe padding). We will use this model as our **policy model πθ**, which will be fine-tuned with GRPO.
- **Architecture Considerations:** Llama-3.2 is a decoder-only transformer suited for causal text generation. It has a context window (likely 8K tokens per Meta’s description ([Meta debuts slimmed-down Llama models for low-powered devices - SiliconANGLE](https://siliconangle.com/2024/10/24/meta-debuts-slimmed-llama-models-low-powered-devices/#:~:text=In%20a%20blog%20post%20today%2C,precision%20of%20their%20model%20weights))) which is more than sufficient for our chain-of-thought prompts and answers. At 3B parameters, it should fit in memory and allow relatively fast experimentation on a 4090. Keep an eye on memory usage though – if running the policy and reward model together becomes heavy, consider using half-precision (already done) or gradient checkpointing on the model (Hugging Face’s `accelerate` can help automate that).
- **Warm-Start with Instruction Tuning:** Since chain-of-thought reasoning can be complex, it's beneficial that we are starting from an instruction-tuned model (or we could even fine-tune the base model on the CoT dataset via supervised learning first). An instruct model will more readily produce step-by-step answers when prompted. If using the raw pretrained model instead, consider doing a supervised fine-tune on the CoT dataset before RL training to give the policy a reasonable starting point.
- **Pitfalls to Avoid:** Ensure the model files downloaded correctly (the 3B model is large, ~6GB in FP16). If loading fails due to memory, use `device_map="auto"` or even offload some layers to CPU. Avoid using too high a model load precision (float32) which doubles memory use unnecessarily. When prompting the model, be mindful of using the same tokenizer for input preparation as used in training (mixing tokenizers can lead to mismatched token IDs). Also, be cautious with the instruct model’s format – some Meta models expect specific prompt formatting (e.g., system/user tokens). Check the model card for any such requirements, or verify by doing a quick generation on a known prompt.

## 3. Preparing the Chain-of-Thought Reasoning Dataset
Load and preprocess the **`moremilk/CoT_Reasoning_Cooking`** dataset which contains chain-of-thought reasoning examples in the cooking domain:
- **Dataset Description:** This dataset consists of question-answer pairs focused on culinary reasoning, where each entry provides a detailed chain-of-thought (CoT) leading to the answer ([moremilk/CoT_Reasoning_Cooking · Datasets at Hugging Face](https://huggingface.co/datasets/moremilk/CoT_Reasoning_Cooking#:~:text=Each%20entry%20goes%20beyond%20simply,understanding%2C%20cooking%20instruction%20generation%2C%20meal)). The questions cover recipe understanding, ingredient relationships, cooking steps ordering, substitutions, and other kitchen logic. Crucially, *each data point includes not just a final answer but the step-by-step reasoning used to arrive at that answer*, illustrating the logical deduction process ([moremilk/CoT_Reasoning_Cooking · Datasets at Hugging Face](https://huggingface.co/datasets/moremilk/CoT_Reasoning_Cooking#:~:text=Each%20entry%20goes%20beyond%20simply,understanding%2C%20cooking%20instruction%20generation%2C%20meal)).
- **Loading the Data:** Use the Hugging Face `datasets` library to load the dataset:
  ```python
  from datasets import load_dataset
  data = load_dataset("moremilk/CoT_Reasoning_Cooking")
  ```
  This likely provides a split (e.g., `data['train']` and possibly a validation set). If no explicit validation split is present, consider manually splitting a portion (e.g., 10%) of the data to use for evaluation and to monitor overfitting.
- **Inspect Structure:** Examine a sample entry. For example:
  ```python
  sample = data['train'][0]
  print(sample.keys(), sample['question'], sample['chain_of_thought'], sample['answer'])
  ```
  The dataset is in JSON format ([moremilk/CoT_Reasoning_Cooking · Datasets at Hugging Face](https://huggingface.co/datasets/moremilk/CoT_Reasoning_Cooking#:~:text=Formats%3A%20%20%20json)); common fields might be `"question"`, `"chain_of_thought"` (the reasoning steps), and `"answer"` (the final conclusion). Confirm how the chain-of-thought and answer are provided (they might be separate fields or the chain-of-thought might include the answer). The dataset card indicates that each entry *“unveils the complete chain-of-thought reasoning behind the conclusions”* ([moremilk/CoT_Reasoning_Cooking · Datasets at Hugging Face](https://huggingface.co/datasets/moremilk/CoT_Reasoning_Cooking#:~:text=Each%20entry%20goes%20beyond%20simply,between%20ingredients%2C%20cooking%20techniques%2C%20and)), so it's likely the reasoning steps are explicitly given.
- **Tokenization & Prompt Formatting:** Prepare the inputs for the model. We want the model to take a question and generate a chain-of-thought and answer. A good approach is to format each prompt in a way that encourages chain-of-thought. For example:
  - **Prompt Template:** `"Question: {question}\nLet's reason this out step by step:\n"` as the input, and expect the model to continue by generating the reasoning and final answer.
  - Alternatively, if the model is instruction-tuned, a simpler prompt like `"{question}\nChain of Thought:"` could suffice. The key is to signal the model to produce reasoning. We can verify a few strategy by testing the model on a sample prompt before training.
- **Encode Inputs:** Use the tokenizer to encode the prompt. Typically, in RLHF training, we will not provide the ground truth answer to the policy model (the model will generate answers itself). But for any supervised pre-training phase or for evaluating model perplexity, we might tokenize the full input + output. For RL rollout, we'll tokenize only the prompt (question plus the fixed prompt cue for CoT) and have the model generate the rest.
- **Collation:** Implement a collation function for the DataLoader that pads sequences to the same length (to the longest prompt in a batch) using the tokenizer’s pad token. This will be needed if we use PyTorch DataLoader for sampling prompts during training.
- **Potential Pitfalls:** Be careful with the prompt formatting – if it's not consistent, the model might sometimes not produce the reasoning. It can help to stick with one prompt format (like always including "Let's reason step by step"). Also, ensure the chain-of-thought in the data is not used directly as a prompt for the model during RL training; we only use it to train the reward model and to evaluate. During RL, the model should generate the chain-of-thought on its own. Finally, watch out for tokenization issues: the Llama tokenizer might treat certain characters specially (for example, ensure newlines are handled properly). Using the same tokenizer for both policy and reward models (if they share vocabulary) is ideal.

## 4. Building a Reward Model for CoT Quality Assessment 
Construct a **reward model** that scores the model's generated reasoning paths for quality, which is essential for RL feedback:
- **Purpose of the Reward Model:** In RLHF, the reward model serves as a stand-in for human judges, providing a scalar score indicating how good a given output is ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=creates%20an%20automated%20way%20to,be%20used%20for%20reinforcement%20learning)). Here, we need a reward model that can evaluate a chain-of-thought answer in the cooking domain and output a higher score for better reasoning (logical, accurate, on-topic) and lower for poor reasoning. This reward signal guides the policy updates.
- **Model Choice:** We will train a reward model from scratch using the Llama 3.2 architecture (or a smaller model for efficiency). One straightforward approach is to initialize another instance of Llama-3.2-3B (or even Llama-3.2-1B to save memory) and attach a small classifier head on top to predict a reward score. Using the same base model means the reward model has the language understanding needed for evaluating answers ([Ray2333/GRM-Llama3.2-3B-rewardmodel-ft · Hugging Face](https://huggingface.co/Ray2333/GRM-Llama3.2-3B-rewardmodel-ft#:~:text=This%20reward%20model%20achieves%20a,surpass%20gpt4%2Fgemini%20as%20a%20judge)). An alternative is to use an existing smaller pretrained model (like DistilBERT or a 1.3B LLM) for the reward model to cut down compute, but given the domain-specific nature (cooking reasoning), leveraging the Llama 3B knowledge is beneficial.
- **Preparing Training Data for Reward Model:** We need to create a set of outputs with quality scores or comparisons to train the reward model. The dataset itself provides only correct chain-of-thought answers (which we assume are high-quality). To train the reward model, we can:
  - Assign the **ground-truth chain-of-thought** a high reward (e.g., label = 1.0 or 5 stars).
  - Generate some **lower-quality responses** for each question to serve as negative examples with lower reward (e.g., label = 0.0). We can use the base policy model (before RL tuning) to produce one or two chain-of-thought answers for each question without looking at the real answer. Many of these generations will be incomplete or incorrect, which we label as poorer quality. Alternatively, we can introduce noise: take the real chain-of-thought and perturb it (remove steps or introduce an error) to simulate a flawed reasoning chain.
  - If pairwise preference data were available, we would train the reward model to predict which of two answers is better, but here we'll simulate it by creating a ranked set (ground truth > generated wrong answer).
- **Training the Reward Model:** Convert the data into a format suitable for supervised training. For example, prepare a list of examples where each example is the concatenation of the **prompt + candidate answer**, and the label is a numerical reward score. We can simply concatenate `"Question: ... \n Answer: ..."` (where "Answer" includes the chain-of-thought reasoning and final answer) as the input text to the reward model. Then:
  - Tokenize these inputs with the same tokenizer (for Llama) including the full question and answer text.
  - Use `AutoModelForSequenceClassification` with a single output (regression) or a small classification head. In practice, for RLHF, a common design is to append a special token (like `<|endoftext|>`) at the end of the answer and have the model output a scalar on that final token's representation ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=creates%20an%20automated%20way%20to,be%20used%20for%20reinforcement%20learning)).
  - Train the reward model on the dataset of (text, score) pairs. Use a loss like MSE (if treating it as regression to target scores) or binary cross-entropy (if labeling good vs bad as 1 vs 0). Given we created pseudo-labels, treat it as relative quality scores.
  - Keep training epochs few (maybe 2-3) and use a small learning rate (e.g., 2e-5) to fine-tune the reward model. The dataset might be on the order of a few thousand prompts, and we generated a couple of outputs each, so tens of thousands of training examples at most.
- **Validation:** If possible, set aside some questions and their true answers, and some model-generated answers, as a validation set to ensure the reward model is learning to rank properly (the true answers should consistently score higher than the fake ones). Monitor the reward model’s performance (accuracy in distinguishing good vs bad, or the regression loss).
- **Using the Reward Model:** Once trained, this model (let's call it **RMφ**) takes a prompt and a candidate chain-of-thought answer and outputs a scalar reward. We will use RMφ to evaluate multiple responses per prompt during GRPO training.
- **Pitfalls & Considerations:** 
  - *Overfitting:* Since the dataset is synthetic and limited, the reward model might overfit to always favor certain writing styles present in the ground truth. To combat this, ensure diversity in negative samples and possibly add slight variety in the positive ones (if multiple correct answers could exist). Also don’t train for too long.
  - *Scaling:* Running a 3B reward model alongside a 3B policy model will roughly double VRAM usage. If this is an issue, use a smaller model for RM (like Llama 1B) or move the reward model to CPU for evaluation (with batched inference to speed up) at the cost of speed. Since 4090 has 24GB, two 3B models in FP16 (~2x6GB) plus overhead should fit, but monitor memory.
  - *Quality of Reward Signal:* The reward model is only as good as the data. Generated negatives might be obviously bad, making the reward model’s job easy; but during RL, the policy will start producing subtler mistakes. The RM might not be perfect at distinguishing fine-grained reasoning quality. This is a known challenge: a misspecified reward model can lead the policy to optimize for the wrong thing (model might learn to game the reward). Keep an eye out during training for any degenerate behavior (like the model producing overly long or repetitive answers just to hit features the RM likes). Using KL regularization (next step) will help mitigate some of this by keeping the model’s outputs reasonable.

## 5. Implementing the Group Relative Policy Optimization (GRPO) Algorithm 
With the policy model and reward model in place, implement the GRPO reinforcement learning procedure to fine-tune the policy:
- **GRPO Overview:** Group Relative Policy Optimization is a variant of PPO that **eliminates the need for a separate value function (critic)** by using *group-based advantage estimation* ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=a%29%20Group)). Instead of training a critic to predict baseline values, GRPO generates **multiple responses per prompt** and uses the average reward of the group as a baseline for advantage calculation ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=a%29%20Group)). This way, each prompt’s responses are only compared against each other, which simplifies training and reduces memory usage (since no extra value network of the same size is needed) ([](https://arxiv.org/pdf/2402.03300#:~:text=Furthermore%2C%20we%20introduce%20the%20Group,Instruct%2C%20including%20both)). GRPO also **integrates a KL-divergence penalty** directly into the loss to keep the new policy close to the reference (initial policy), enhancing training stability ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=b)).
- **Algorithm Steps per Iteration:** For each training step (or episode):
  1. **Prompt Sampling:** Sample a batch of prompts (questions) from the dataset (e.g., batch of N prompts).
  2. **Response Generation:** For each prompt in the batch, have the policy model generate *k* distinct responses (chain-of-thought answers) using stochastic sampling (e.g., top-p or top-k sampling with some temperature). For example, choose k = 4 or 8 responses per prompt. This gives a *group* of responses per prompt.
  3. **Reward Scoring:** For each generated response, use the reward model (RMφ) to score it. This yields rewards r₁, r₂, ..., r_k for the group of responses to that prompt.
  4. **Compute Baseline:** Compute the **baseline b** for that prompt as the mean reward of the group: b = (r₁ + ... + r_k) / k ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=a%29%20Group)). This baseline is essentially an estimate of an “average” answer quality for that question, which replaces the value function in PPO.
  5. **Advantage Calculation:** For each response i, compute its advantage Aᵢ = rᵢ – b. This tells us which responses were better than average (positive advantage) or worse (negative) for that prompt.
  6. **Policy Update (Loss):** We now update the policy using these advantages. For each prompt response pair, we have the log-probability of the policy generating that response (more precisely, the log-prob of the sequence of tokens). Let πθ(old) be the policy before update and πθ(new) the current policy (θ are model parameters). The GRPO objective can be derived from PPO:
     - We maximize the expectation of advantage * probability ratio. The probability ratio is `ρ = πθ(new)(response|prompt) / πθ(old)(response|prompt)`. The PPO clipped objective would try to maximize *ρ · Aᵢ* while keeping ρ constrained (to avoid too large updates) ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=Now%2C%20PPO%20is%20a%20policy,large%20deviations%2C%20ensuring%20stable%20training)).
     - **Clipping:** Apply the PPO-style clipping: limit ρ between [1-ε, 1+ε] (say ε ~ 0.2) so that if the policy tries to change too much on one update, the advantage is capped. This ensures conservative updates for stability.
     - **KL Regularization:** Incorporate a penalty for deviating from the reference policy (which can be the initial SFT model or the previous policy state). GRPO explicitly adds the KL divergence between the current policy and a reference policy into the loss ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=b)). Practically, this can be done by adding an extra term: **loss_KL = β * KL( πθ(new) || π_ref )**, where β is a coefficient controlling how strongly to restrain the policy. The reference π_ref is often the initial model (to keep the model from drifting too far from human-like responses). Some implementations use the initial policy (before RL) as a fixed reference, others update a reference model periodically. In our case, we can keep the pre-trained instruct model fixed as π_ref throughout RL training and measure KL against it.
     - The total policy loss for each response can be: **L = - E[ clip(ρ, 1-ε, 1+ε) * Aᵢ ] + β * KL(πθ(new)||π_ref)** (where the expectation is taken over the batch of prompt-response pairs). We take a gradient step to minimize this loss (note the negative sign on the advantage term since we want to maximize reward).
  7. **Optimization:** Use an optimizer (Adam or AdamW) on this loss to update the policy model weights. If using the TRL `GRPOTrainer`, a lot of this is handled internally – we would provide the reward model and reference model, and it will take care of generating samples, computing rewards and advantages, and doing updates.
- **Hyperparameters:** Decide on key hyperparameters:
  - *Batch size:* Maybe 8 prompts per batch (with k responses each, that’s 8*k forward passes per step). Adjust based on GPU memory.
  - *Group size (k):* Commonly 4–8. Higher k gives a better estimate of the baseline but means more generation per prompt (slower per update). Start with k=4.
  - *Sequence length:* Limit the maximum tokens generated for each chain-of-thought (maybe 256 tokens) to avoid runaway generations.
  - *KL coefficient (β):* A typical value might be 0.1 to 0.2 to start. This might need tuning; if you observe the model outputs deviating too much (becoming incoherent or off-topic even if reward is high), increase β to pull it back toward reference behavior. If the model is learning too slowly or not improving (perhaps staying too safe), β might be lowered.
  - *Learning rate:* Use a relatively small LR for policy (e.g., 1e-5 or 2e-5) since a high LR in RLHF can cause divergence, especially with a large model. Also consider using a scheduler or adaptive LR.
  - *Clip range (ε):* 0.2 is standard from PPO ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=Now%2C%20PPO%20is%20a%20policy,large%20deviations%2C%20ensuring%20stable%20training)), but monitor if clipping is happening too often or not at all and adjust if necessary.
  - *Epochs/Updates:* Rather than traditional epochs, RL training might be run for a certain number of steps (e.g., 1000 steps) or until reward saturates. Each step processes a batch of prompts.
- **Logging:** Keep track of key metrics at each iteration:
  - Mean and standard deviation of rewards in each batch (so you can see if rewards are improving).
  - KL divergence between policy and reference (ensure it’s not growing unchecked).
  - Possibly track the average advantage or the percentage of clipped updates.
  - Sample outputs periodically to see qualitative progress.
- **Pitfalls:** 
  - *High Variance:* Using a group baseline reduces variance compared to a single-sample advantage, but with small k the estimates can still be noisy. If training is unstable, increasing k or using multiple batches to estimate baseline could help at cost of speed.
  - *Mode Collapse:* Sometimes the policy may collapse to always output a certain style that the reward model likes (since no diversity penalty other than KL). If you notice outputs becoming very repetitive or similar, that’s a warning sign. Mitigate by tuning KL or even adding a entropy bonus (encourage the policy to keep outputting diverse tokens).
  - *Reward Model Exploitation:* The policy might find a trick to game the reward model (for example, always using a certain phrase that the RM wrongly associates with good answers). Keep an eye on whether the improved reward is translating to genuinely better reasoning or just higher RM scores. If it’s the latter, consider refining the reward model with more data or regularizing the policy updates more strongly.

## 6. Fine-Tuning the Llama3.2 Model with GRPO 
Execute the GRPO training loop to fine-tune the policy model on the CoT reasoning task:
- **Initialize Training:** If using the TRL `GRPOTrainer`, instantiate it with the policy model, the reward model, the reference model for KL (this could just be a fixed copy of the policy model’s initial weights), and the dataset of prompts. Configure the trainer with our hyperparameters (batch size, learning rate, kl_coeff, etc.) and a collate function to format prompts. The trainer will handle sample generation and optimization internally.
  ```python
  from trl import AutoModelForCausalLMWithValueHead, GRPOTrainer
  # Note: AutoModelForCausalLMWithValueHead might not be needed since GRPO forgoes value head.
  trainer = GRPOTrainer(
      model=policy_model,
      ref_model=reference_model,
      tokenizer=tokenizer,
      reward_model=reward_model,
      dataset=prompt_dataset,
      # batch_size, learning_rate, etc.
      max_length=256, 
      gen_kwargs={"temperature": 1.0, "top_p": 0.95, "do_sample": True, "max_new_tokens": 256},
      k=4,
      epsilon=0.2,
      beta=0.1
  )
  ```
  Under the hood, this will generate k responses for each prompt in a batch, compute rewards, advantages, and perform updates according to GRPO.
- **Manual Loop (if not using Trainer):** If implementing manually, loop through training steps:
  - Sample batch of prompts.
  - Generate responses (use `model.generate()` with specified decoding params to get k outputs per prompt).
  - Compute rewards via `reward_model` (likely do this in a no-grad context).
  - Calculate losses as described and call `optimizer.step()`. You would need to reshape the data properly since we have k responses per prompt (essentially treat each prompt-response pair as an independent trajectory for the loss, but note that advantages are computed group-wise).
  - Periodically update learning rate or KL coefficient if needed based on progress.
- **Stability Monitoring:** Training may require some experimentation:
  - If you see the policy loss or KL term exploding, consider reducing the learning rate or increasing β (to constrain updates).
  - If rewards aren’t improving at all in the first few hundred steps, the model might be stuck – possibly the reward signal is not strong enough or learning rate is too low. Ensure that the reward model is providing a gradient (if all advantages are near zero, maybe because every response got similar reward, then try increasing the variability of responses or tweaking reward model sensitivity).
  - Track the KL divergence: GRPO explicitly tries to keep the policy close to the reference. If KL divergence (between policy and reference) remains very small, the model might not be exploring better answers enough – you might reduce β to allow more deviation. Conversely, if KL is increasing steadily and the model’s outputs start looking off-track, increase β.
- **Checkpointing:** Save model checkpoints at intervals. For example, after every 100 training steps or when an improvement is observed, save `policy_model` (its state_dict or use `save_pretrained`). This is important in case training diverges, you can rollback to a good state. Also, it allows later evaluation of intermediate models.
- **Training Duration:** Fine-tune until you see convergence or satisfying performance. This could be a few hundred to a couple thousand steps given the dataset size. Because each step uses fresh samples, you’re effectively doing many epochs over the prompt dataset (with different generated answers each time). Watch the reward trend: it may plateau when the model has learned to consistently produce high-quality reasoning.
- **Pitfalls:** RL training can be unstable. Some specific pitfalls to avoid:
  - *Updating too fast:* Applying too many optimization steps per batch of samples (or too high learning rate) can quickly distort the policy. Stick to one update per batch of freshly generated samples (don’t reuse the same samples for multiple gradient steps without generating new ones, to avoid overfitting to those specific examples).
  - *Ignoring the Reference Update:* If using a fixed reference (initial model) for KL, note that over long training the policy might drift far even if slowly – if KL gets too high, the outputs might become less sensible. One trick used in some implementations is to periodically reset the reference model to the current policy (so that KL is always computed to a relatively recent policy, rather than a very old one). However, this deviates from the original paper which likely keeps reference fixed. Monitor and decide if needed.
  - *Resource Exhaustion:* Generating many samples is GPU-intensive. Ensure the generation is vectorized if possible (the TRL trainer should do this) – i.e., generate k responses for N prompts in parallel if memory allows, rather than one by one. Use Torch CUDA ops for speed and avoid Python loops in the inner generation where possible. Also, free up GPU memory of the reward model after use if doing sequentially (although with 24GB, parallel should be fine). If using multiple processes or accelerate’s distributed mode, be careful to avoid multiple copies of the model beyond what you intend.
  - *Evaluating in Training:* Avoid using the reward model’s score as the sole measure of success in training (since the policy is explicitly optimizing for it). Always perform separate evaluation (next section) to gauge real improvements. The reward model score going up means the policy is doing what the reward model likes, but we want to ensure that corresponds to genuinely better chain-of-thought reasoning, not just overfitting quirks of the reward model.

## 7. Evaluation of Pre- and Post-GRPO Performance 
After training, evaluate the fine-tuned model to verify improvements in chain-of-thought reasoning on the cooking dataset:
- **Baseline vs Fine-Tuned:** First, take the original model (before GRPO) and the GRPO-tuned model and have each generate answers for a set of questions. Ideally use a test split from the dataset (or some questions not seen during training). For consistency, use the same prompting format for both models (e.g., `"Question: ...\nLet's reason step by step:\n"` and let the model generate).
- **Quality of Chain-of-Thought:** Compare the outputs:
  - **Logical Coherence:** Does the GRPO-trained model produce a more logically structured reasoning chain? For example, are the steps in correct order (first gather ingredients, then mix, then cook, etc.)? The dataset’s focus is on *“structured logical steps”* in cooking processes ([moremilk/CoT_Reasoning_Cooking · Datasets at Hugging Face](https://huggingface.co/datasets/moremilk/CoT_Reasoning_Cooking#:~:text=Each%20entry%20goes%20beyond%20simply,understanding%2C%20cooking%20instruction%20generation%2C%20meal)), so check if the model’s steps align with that structure better than the baseline.
  - **Accuracy of Conclusions:** Does the final answer at the end of the chain-of-thought correctly answer the question (e.g., if the question asks for a cooking time or substitution, is it correct)? The ground truth answers in the dataset can be used as a reference for correctness.
  - **Depth of Reasoning:** Is the fine-tuned model providing deeper or more detailed explanations than before? GRPO training should encourage thorough reasoning to maximize reward, so you might see the model giving more detailed answers covering all aspects of the question.
- **Quantitative Metrics:** If the dataset has a well-defined correct answer for each question, you can compute an **accuracy** or correctness rate for final answers. For example, count how many answers exactly match or are semantically equivalent to the expected answer. If the answers are free-form text, use a similarity metric or a simple heuristic to judge correctness.
  - You can also use the **reward model** as an automated evaluator on these test outputs. Feed the test questions and each model’s answer into the reward model: does the GRPO model’s answers consistently score higher than the baseline’s? This would indicate the reward model (and presumably the training objective) aligns with improvements.
  - Another metric is the average **reward** (according to RM) per response on test prompts, pre- vs post-training. Since that is what we optimized, it should be higher for the fine-tuned model.
  - If available, use external evaluation datasets or benchmarks for chain-of-thought reasoning. For example, although domain-specific, one might see if the model’s reasoning style works well on generic reasoning tasks or if there’s any unintended degradation.
- **Example Comparison:** It can be illuminating to present a few example Q&A with chain-of-thought from both models:
  - *Question:* "If a recipe calls for sautéing onions before adding tomatoes, why is this order important?"
    - Baseline answer might be short or generic.
    - Post-GRPO answer might lay out: "First, onions are sautéed to caramelize and develop sweetness. If tomatoes (which release liquid) were added first, the onions would simmer rather than brown. Therefore, the order is important to build flavor." – showing a clear stepwise explanation.
  - Look for such qualitative differences that align with improved reasoning.
- **Check for Side Effects:** Ensure that the fine-tuned model hasn’t developed unwanted behaviors:
  - Is it still following instructions properly (not just answering in a fixed style)? 
  - Are the answers overly lengthy or verbose now? Sometimes RLHF-tuned models can become too verbose in an attempt to please the reward model. If so, this might be tuned by adjusting the prompt or adding brevity reward in future.
  - Is the model making fewer factual errors about cooking? Ideally, by focusing on reasoning, it should also avoid logical mistakes.
- **Documentation of Results:** Record the performance differences. For instance, “The fine-tuned model achieved 90% accuracy on the test questions versus 75% for the base model, and the average reward model score increased from 0.6 to 0.9. Qualitatively, the GRPO-tuned model provides more detailed and correct step-by-step solutions in the cooking domain.”
- **Iterate if Necessary:** If the improvements are modest, you may revisit the training: perhaps increase the number of training steps, refine the reward model (maybe the reward model wasn’t judging perfectly), or adjust the KL penalty to allow more exploration. This evaluation step is not just for reporting but also to guide further refinement.
- **Conclusion:** Summarize that using GRPO, we successfully fine-tuned Llama3.2 3B to significantly enhance its chain-of-thought reasoning ability on the cooking dataset. GRPO’s group-based advantage and KL-regularized updates allowed stable training without a value network ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=Summary%20of%20why%20GRPO%20is,Better)) ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=2.%20Stability%3A%20The%20group,integration%20make%20training%20more%20stable)), efficiently leveraging the reward model’s judgments to improve the policy. The result is a model that better explains its answers and demonstrates improved logical reasoning in its responses, as evidenced by both automated metrics and qualitative analysis.

**Sources:**

- Llama 3.2 Model Card – Meta’s 3B parameter model details ([meta-llama/Llama-3.2-3B - Hugging Face](https://huggingface.co/meta-llama/Llama-3.2-3B#:~:text=The%20Llama%203,in%201B%20and%203B%20sizes)) ([Meta debuts slimmed-down Llama models for low-powered devices - SiliconANGLE](https://siliconangle.com/2024/10/24/meta-debuts-slimmed-llama-models-low-powered-devices/#:~:text=models%20more%20accessible%20with%20the,powered%20devices))  
- CoT Cooking Dataset Description – Emphasis on chain-of-thought reasoning in culinary tasks ([moremilk/CoT_Reasoning_Cooking · Datasets at Hugging Face](https://huggingface.co/datasets/moremilk/CoT_Reasoning_Cooking#:~:text=Each%20entry%20goes%20beyond%20simply,understanding%2C%20cooking%20instruction%20generation%2C%20meal))  
- RLHF and Reward Model – Use of reward models to convert human preferences to a reward signal ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=creates%20an%20automated%20way%20to,be%20used%20for%20reinforcement%20learning))  
- GRPO Algorithm – Group-based advantage (no critic) and KL regularization in policy optimization ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=a%29%20Group)) ([AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO) ](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo?lang=en#:~:text=b))  
- DeepSeekMath (GRPO) – GRPO foregoes critic by using group score baseline, improving training efficiency