# Sprint 16 Concepts: RLHF, Reward Models, and GRPO\n\nThis sprint focuses on fine-tuning a Llama 3.2 model using **Group Relative Policy Optimization (GRPO)**, a technique rooted in **Reinforcement Learning from Human Feedback (RLHF)**. Here\'s a breakdown of the key concepts:\n\n## 1. Reinforcement Learning from Human Feedback (RLHF)\n\nRLHF is a multi-stage process designed to align Large Language Models (LLMs) more closely with human preferences and desired behaviors (like helpfulness, harmlessness, or following specific instructions like Chain-of-Thought reasoning).\n\nThe typical stages are:\n\n1.  **Supervised Fine-Tuning (SFT):** Start with a base pre-trained LLM and fine-tune it on a high-quality dataset of prompt-response examples demonstrating the desired behavior (e.g., helpful answers, specific formats). This results in an SFT model.\n2.  **Reward Modeling:** Train a separate model (the Reward Model) to predict which of two responses to the same prompt a human would prefer.\n3.  **RL Optimization:** Use a reinforcement learning algorithm (like PPO or, in our case, GRPO) to further fine-tune the SFT model (now called the \"policy model\"). The policy model generates responses, the reward model scores them, and the RL algorithm updates the policy model to maximize the scores given by the reward model.\n\n## 2. Reward Model (RM) - Task 4\n\n*   **Purpose:** The RM acts as a proxy for human preference. Its job is to take a prompt and a generated response and output a scalar score indicating how \"good\" that response is according to the preferences learned during its training.\n*   **Training Data:** The RM is typically trained on a dataset of comparisons. For a given prompt, humans (or sometimes another AI) label which of two generated responses is better. Our `prompt`, `chosen`, `rejected` dataset structure is perfect for this.\n*   **Training Goal:** The RM learns to assign a higher score to the `chosen` response than the `rejected` response for the same `prompt`.\n*   **Architecture:** Often, the RM is based on the same architecture as the policy model (or a slightly modified version), initialized from the SFT checkpoint, with a final linear layer added to output the scalar reward score.\n\n## 3. Group Relative Policy Optimization (GRPO) - Task 5\n\nGRPO is an RL algorithm used in the final stage of RLHF. It aims to improve upon older methods like Proximal Policy Optimization (PPO) by being more computationally efficient, which is crucial for large models.\n\n*   **Key Idea:** Instead of requiring *four* models during RL training (policy, reference, reward, and value models, as in PPO), GRPO cleverly **eliminates the need for the value model**.\n*   **How it Works:**\n    1.  **Generate a Group:** For a given prompt from a batch, the current policy model (the LLM we are training) generates *multiple* responses (e.g., k=4 or k=8 responses).\n    2.  **Score the Group:** The **Reward Model (RM)** scores each of these k responses.\n    3.  **Calculate Relative Advantage:** The \"advantage\" of each response is calculated based on its reward *relative* to the average reward of the entire group generated for that prompt. \( Advantage_i = R_i - \frac{1}{k} \sum_{j=1}^{k} R_j \). (Often normalized by standard deviation too).\n    4.  **Policy Update:** The policy model's weights are updated to increase the probability of generating responses with a positive relative advantage (i.e., better than the group average for that prompt) and decrease the probability of those with a negative relative advantage.\n    5.  **KL Divergence Regularization:** Like PPO, GRPO includes a KL divergence term in its loss function. This measures how much the current policy model has diverged from a frozen **reference model** (usually the initial SFT model). This term penalizes large deviations, preventing the model from forgetting its general language abilities while optimizing for the specific reward.\n*   **Efficiency:** By removing the value model (which often has a similar size to the policy model), GRPO significantly reduces the memory and computational requirements for RLHF training.\n\n## Connection to Our Dataset\n\n*   Our dataset (`prompt`, `chosen`, `rejected`) created in Task 3 is primarily used for **training the Reward Model (Task 4)**. The RM learns the preference function: `score(prompt, chosen) > score(prompt, rejected)`.\n*   During the **GRPO training step (Task 5)**, we will use this trained Reward Model to score the groups of responses generated by the policy model and calculate the relative advantages needed to update the policy.\n 