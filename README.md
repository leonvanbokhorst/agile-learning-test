# Project: Learn PyTorch & Build GPT-2 from Scratch

This project uses the Agile Learning framework (see `AGILE_FRAMEWORK.md`) to structure the process of learning PyTorch in-depth and building a GPT-2 like language model from the ground up.

## ðŸŽ¯ Epic: Master PyTorch Fundamentals and Construct a Foundational Transformer LLM

**Vision Statement:**
Gain a robust, practical understanding of PyTorch by implementing core deep learning concepts, culminating in the construction and basic training of a simplified GPT-2 style transformer model. This involves understanding the theory behind each component and translating it into working PyTorch code.

**Key Objectives / Success Criteria:**

- [ ] **PyTorch Proficiency:** Demonstrate understanding and fluent use of PyTorch tensors, autograd, modules (`nn.Module`), optimizers, and data loading (`Dataset`, `DataLoader`).
- [ ] **Transformer Architecture:** Implement the key components of a transformer model (embedding layers, positional encoding, multi-head self-attention, feed-forward networks, layer normalization) in PyTorch.
- [ ] **Working Model:** Assemble the components into a functional GPT-2 like model capable of processing input sequences and generating output sequences.
- [ ] **Basic Training Loop:** Implement a training loop to train the model on a small dataset, demonstrating understanding of loss calculation, backpropagation, and parameter updates.
- [ ] **Conceptual Understanding:** Be able to explain the purpose and function of each model component and the overall training process.

## ðŸš€ Current Status

See the `sprints/` directory for active and completed work blocks.
For a detailed breakdown of completed sprints and overall progress, see [`milestones.md`](milestones.md).
